# AFML: pp 113 - 126

### Ch 8: Feature Importance

##### 8.1 Motivation

- ç®€å•æ¥è¯´å°±æ˜¯å¦‚æœè¯•äº†å¾ˆå¤šæ¬¡ï¼ˆ>20æ¬¡ï¼‰ï¼Œæ‰¾åˆ°ä¸€ä¸ªsignificance > 5%çš„ï¼Œå…¶å®å¹¶ä¸èƒ½ç”¨ã€‚å¦‚æœå°è¯•çš„æ¬¡æ•°å¤šï¼Œsignificanceè¿™ä¸ªthreasholdåº”è¯¥ææ›´é«˜ï¼Œæ¯”å¦‚1%ç­‰

##### 8.2 The importance of feature importance

æ‰¾featureçš„æ—¶å€™å€¼å¾—é—®çš„é—®é¢˜ï¼š

- Are these features important all the time, or only in some specific environments?
- What triggers a change in importance over time?
- Can those regime switches be predicted?
- Are those important features also relevant to other related financial instruments?
- Are they relevant to other asset classes?
- What are the most relevant features across all financial instruments?
- What is the subset of features with the highest rank correlation across the entire investment universe?

æ€è€ƒè¿™äº›é—®é¢˜æ¯”foolish backtest cycleå¥½å¤šäº†ã€‚

##### 8.3 Feature Importance with Substitution Effects

Substitution effects: ç›¸å½“äº multi-collinearity

- è§£å†³æ–¹æ¡ˆï¼šPCA

##### 8.3.1 Mean Decrease Impurity (MDI)

- Explanatory-importance (in-sample, IS) metod => tree-based classifiers
- å¦‚æœæœ‰å¾ˆå¤šæ£µæ ‘ï¼Œå¯ä»¥æŠŠå„ä¸ªæ ‘é‡Œé¢featureçš„overall impurity decreaseå–ä¸ªå¹³å‡å€¼ç„¶årank ä¸€ä¸‹featureçš„importance

ç”¨MDIæ—¶å€™éœ€è¦æ³¨æ„çš„ç‚¹ï¼š

1. å¦‚æœä¸€äº›featureæ²¡æœ‰è¢«treeé€‰ä¸­ï¼Œä¼šæœ‰Masking effectsï¼Œå³å®ƒçš„impurity decreaseæ˜¯0
    - ç”¨sklearnçš„RFæ˜¯è®¾ç½®`max_features=int(1)` ï¼Œæ¯ä¸ªlevelåªæœ‰ä¸€ä¸ªrandom featureè¢«é€‰ä¸­
    - æ¯ä¸ªfeatureéƒ½æœ‰æœºä¼šè¢«é€‰ä¸­å»split node
    - å¦‚æœæŸä¸€æ£µæ ‘é‡Œfeature importanceæ˜¯0ï¼Œæ‹¿å»è®¡ç®—average impurity decreaseçš„æ—¶å€™å¿½ç•¥è¿™ä¸ª0
2. procedureæ˜¯in-sampleçš„ï¼Œæ¯ä¸ªfeatureå³ä½¿æ²¡æœ‰predictive powerä¹Ÿæœ‰ä¸€äº›importance
3. MDIæ²¡æ³•ç”¨åœ¨å…¶ä»–non-tree based classifierä¸Š
4. MDIæœ‰ä¸ªå¥½å¤„æ˜¯æ¯ä¸ªfeatureçš„importanceåœ¨0-1ï¼Œä¸”åŠ èµ·æ¥æ˜¯1
5. MDIæ²¡æœ‰è€ƒè™‘substitution effectsï¼Œè€Œä¸”ä¼šå‰Šå‡å…±çº¿æ€§çš„featureçš„importance
6. å¯èƒ½ä¼šbiased toæœ‰å¾ˆå¤šcategoryçš„feature

- Sklearnçš„RandomForesté»˜è®¤çš„feature importance metricå°±æ˜¯MDI

##### 8.3.2 Mean Decrease Accuracy (MDA)

- slow, predictive-importance (out-of-sample, OOS) method
- ç¬¬ä¸€æ­¥ï¼Œè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼› ç¬¬äºŒæ­¥ï¼Œæ ¹æ®ä¸€äº›metricsï¼ˆAccã€negative log=lossï¼‰deriveå‡ºå®ƒçš„out-of-sample performanceï¼› ç¬¬ä¸‰æ­¥ï¼Œå¯¹featureçŸ©é˜µ (X) çš„æ¯ä¸€åˆ—è¿›è¡Œç½®æ¢ï¼Œä¸€æ¬¡ä¸€åˆ—ï¼Œåœ¨æ¯ä¸€åˆ—ç½®æ¢åå¾—å‡ºout-of-sample performanceã€‚ featureçš„é‡è¦æ€§æ˜¯ function of å…¶column's permutation å¼•èµ·çš„performance lossã€‚

æ³¨æ„ç‚¹ï¼š

1. é€‚åˆä»»ä½•classifierï¼Œä¸ä¸€å®šæ˜¯tree-based classifier
2. ä¸æ­¢é€‚åˆAccï¼Œè¿˜é€‚åˆå…¶ä»–metricsï¼Œæ¯”å¦‚F1
3. è¿˜æ˜¯å®¹æ˜“å—subsitution effectå½±å“ã€‚ç»™ä¸¤ä¸ªä¸€æ ·çš„featureï¼Œå®ƒä¼šè®¤ä¸ºå…¶ä¸­ä¸€ä¸ªæ˜¯å®Œå…¨å¯ä»¥è¢«å¦ä¸€ä¸ªæ›¿ä»£çš„ï¼ˆç¡®å®ï¼‰ï¼Œä½†ä¹ŸåŒæ ·ä¼šè®¤ä¸ºè¿™ä¸¤ä¸ªfeatureéƒ½æ²¡ç”¨ï¼Œå³ä½¿ä»–ä»¬å®é™…ä¸Šæ˜¯æœ‰ç”¨çš„
4. MDAå¯èƒ½ä¼šè®¤ä¸ºæ‰€æœ‰featureéƒ½æ˜¯unimportantçš„
5. Cross validationéœ€è¦purge and embargo

##### 8.4 Feature Importance without Substitution Effects

##### 8.4.1 Single Feature Importance (SFI)

- Cross-section predictive importance (OOS)

æ³¨æ„ç‚¹ï¼š

1. ä¸å±€é™äºtree
2. ä¸åªæ˜¯accuracy
3. ä¸å—substitution effectså½±å“
4. å¯èƒ½è®¤ä¸ºæ‰€æœ‰featureéƒ½æ˜¯unimportantçš„

ç¼ºé™·ï¼š

1. æ²¡æ³•è¯´æ˜joint importanceï¼Œæ¯”å¦‚feature Båªæœ‰åœ¨feature Aä¹Ÿå­˜åœ¨çš„æ—¶å€™importanceï¼Œç”¨SFIå°±å‘ç°ä¸äº†äº†
    - ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ˜¯è®¡ç®—subset of featuresçš„OOS performance score

##### 8.4.2 Orthogonal Features

- ç¼“è§£substitution effects => orthogonalize the features (æ¯”å¦‚PCA)

æ­¥éª¤ï¼š

1. ç‰¹å¾çŸ©é˜µ$\{X_{t,n}\}$ standardize => Z
2. è®¡ç®—Zçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ $Z'ZW=W\Lambda$
3. ç”Ÿæˆæ­£äº¤çš„ç‰¹å¾ï¼š$P=ZW$
    - $P'P=W'Z'ZW=W'W\Lambda W'W=\Lambda$

å…ˆstandardizationçš„æ„ä¹‰ï¼š

- centering the data: ä½¿å¾—first principal component oriented in the main direction of the observation
- rescaling: ä½¿å¾—PCAå…³æ³¨äºcorrelationï¼Œå¦‚æœä¸è¿™æ ·çš„è¯ï¼Œfirst principal components è¢«varianceæœ€å¤§çš„feature dominate

ç”¨orthogonalçš„ä¸€å¤§å¥½å¤„æ˜¯

- ä½ å¯ä»¥å»éªŒè¯ä½ ç”¨æ¯”å¦‚SFIã€MDAæ‰¾çš„featureçœŸçš„æ˜¯importantçš„ï¼ˆå®ƒä»¬ä¸€èµ·ç»„æˆäº†PCAæ‰¾åˆ°çš„importantçš„featureçš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼‰ğŸŒŸ

è®¡ç®—correlation between PCA rankçš„featureå’ŒMDIç­‰å…¶ä»–æ–¹æ³•æ‰¾åˆ°çš„featureï¼š

- ç”¨weighted Kendall's tauæ›´å¥½

##### 8.5 Parallelized vs. Stacked Feature Importance

æ–¹æ³•1: è®¡ç®—æ¯ä¸ªsecurity $(X_i,y_i)$å¯¹åº”çš„featureçš„importanceï¼Œç„¶åaverage across different securitieså¾—åˆ°è¿™ä¸ªfeatureçš„importance

- parallelize ç®—èµ·æ¥æ¯”è¾ƒå¿«
- ä¼šå—substitution effectçš„å½±å“ï¼Œfeature rankä¼šæœ‰ä¸€äº›å˜åŠ¨ï¼Œå¢åŠ äº†varianceï¼Œä¸è¿‡averageä¹‹åå¯ä»¥å¿½ç•¥

æ–¹æ³•2: æŠŠsecuritiesç»„æˆä¸€ä¸ªå¤§çš„matrix $(X,y)$ï¼Œå…¶ä¸­$\tilde{X}_i$ æ˜¯transformed instance of $X_i$ (æ¯”å¦‚standardized on a rollling trailing window => ensure some distributional homogeneity $\tilde{X}_i\sim X$)ï¼Œç„¶åç›´æ¥è®¡ç®—importance

- ä¸éœ€è¦weighting scheme
- æ›´generalï¼Œless biased by å¼‚å¸¸å€¼å’Œoverfitting
- å› ä¸ºæ²¡æœ‰æŠŠimportance score averageï¼Œæ‰€ä»¥substitution effectä¸ä¼šè®©è¿™äº›scoreé™å¾ˆå¤š
- stackingæ›´å¥½ä¸€ç‚¹ï¼Œå› ä¸ºå‡å°‘äº†overfitting æŸä¸€ä¸ªsecurityçš„å¯èƒ½ï¼Œç¼ºç‚¹å°±æ˜¯éœ€è¦æ›´å¤šçš„memory

##### 8.6 Experiments with Synthetic Data

ä¸€äº›ä»£ç ï¼Œå…ˆç•¥ã€‚

# notes

信息熵，就是最基础的

条件熵，条件熵描述了在已知第二个随机变量 X 的值的前提下，随机变量 Y 的信息熵还有多少

互信息，度量了两个变量之间相互依赖的程度。具体来说，对于两个随机变量，MI是一个随机变量由于已知另一个随机变量而减少的“信息量

交叉熵 cross entropy

KL 散度，衡量两个分布之间的距离，但是它并不是对称的，ab 距离不一定等于 ba 距离

自信息，自信息的期望值就是信息论中的熵，它反映了随机变量采样时的平均不确定程度。

好多概念
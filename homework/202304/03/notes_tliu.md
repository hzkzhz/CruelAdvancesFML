# notes

Bagging是个体学习器不存在强依赖关系，可以同时生成的并行化方法；Boosting是个体学习器间存在强依赖关系、必须串行生成的序列化方法。
从偏差-方差分解角度看，Bagging主要关注降低方差，而Boosting主要关注降低偏差。

看到了这样一句话，感觉非常合理。
我们上一节大概讲了一下决策树是什么，现在随机森林就是类似于一个 bagging 的决策树，然后我们最终的结果由诸多决策树的众数决定。

决策树在Bagging集成的基础上，进一步在决策树的训练过程中加入了随机属性的选择。具体来说，传统决策树在选择划分属性时是在当前结点的所有候选属性（假定有d个）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的候选属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。抽取的属性数k的选择比较重要，一般推荐log(d)。由此，随机森林的基学习器的“多样性”不仅来自样本的扰动，还来自属性的扰动，使得最终集成的泛化能力进一步增强。